{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0204d5",
   "metadata": {
    "id": "ijHnNTIjDkt0",
    "lines_to_next_cell": 2
   },
   "source": [
    "# Train a neural network for binary volumetric brain extraction\n",
    "\n",
    "In this notebook, we will use the `nobrainer` python API to train a model for brain extraction. Brain extraction is a common step in processing neuroimaging data. It is a voxel-wise, binary classification task, where each voxel is classified as brain or not brain.\n",
    "\n",
    "In the following cells, we will:\n",
    "\n",
    "1. Get sample T1-weighted MR scans as features and FreeSurfer segmentations as labels.\n",
    "2. Convert the data to TFRecords format for use with neural networks.\n",
    "3. Create two `Datasets` of features and labels, one for training, one for evaluation.\n",
    "4. Instantiate a 3D convolutional neural network model for image segmentation (U-Net).\n",
    "5. Train on part of the data and evaluate on the rest of the data.\n",
    "6. Predict a brain mask using the trained model.\n",
    "7. Save the model to disk for future prediction and/or training.\n",
    "8. Load the model back from disk and show that brain extraction works as before saving.\n",
    "\n",
    "\n",
    "## Google Colaboratory\n",
    "\n",
    "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a777f39",
   "metadata": {
    "id": "WhBnt2WdDlx9",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!pip install nobrainer nilearn\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import nobrainer\n",
    "\n",
    "csv_path = nobrainer.utils.get_data()\n",
    "filepaths = nobrainer.io.read_csv(csv_path)\n",
    "\n",
    "from nobrainer.dataset import Dataset\n",
    "\n",
    "n_epochs = 2\n",
    "DT = Dataset(n_classes=1,\n",
    "             batch_size=2,\n",
    "             block_shape=(128, 128, 128),\n",
    "             n_epochs=n_epochs)\n",
    "\n",
    "dataset_train, dataset_eval = DT.from_files(\n",
    "    paths=filepaths,\n",
    "    eval_size=0.1,\n",
    "    tfrecdir=\"data/binseg\",\n",
    "    shard_size=3,\n",
    "    augment=None,\n",
    "    shuffle_buffer_size=10,\n",
    "    num_parallel_calls=None,\n",
    ")\n",
    "\n",
    "from nobrainer.processing.segmentation import Segmentation\n",
    "from nobrainer.models import unet\n",
    "\n",
    "bem = Segmentation(unet, model_args=dict(batchnorm=True), multi_gpu=True)\n",
    "\n",
    "history = bem.fit(\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_validate=dataset_eval,\n",
    "    epochs=n_epochs,\n",
    ")\n",
    "\n",
    "bem.save(\"data/unet-brainmask-toy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88200c",
   "metadata": {},
   "source": [
    "## Load the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f979c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "bem = Segmentation.load(\"data/unet-brainmask-toy\", multi_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa063b",
   "metadata": {},
   "source": [
    "## Restart training where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bem.fit(\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_validate=dataset_eval,\n",
    "    epochs=n_epochs,\n",
    "    warm_start=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
