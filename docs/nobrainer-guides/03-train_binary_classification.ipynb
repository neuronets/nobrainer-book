{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijHnNTIjDkt0",
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a neural network for binary volumetric brain mask segmentation\n",
    "\n",
    "In this notebook, we will use Nobrainer to train a model for brain extraction. Brain extraction is a common step in processing neuroimaging data. It is a voxel-wise, binary classification task, where each voxel is classified as brain or not brain.\n",
    "\n",
    "In the following cells, we will:\n",
    "\n",
    "1. Get sample T1-weighted MR scans as features and FreeSurfer segmentations as labels.\n",
    "    - We will binarize the FreeSurfer to get a precise brainmask.\n",
    "2. Convert the data to TFRecords format.\n",
    "3. Create two Datasets of the features and labels.\n",
    "    - One dataset will be for training and the other will be for evaluation.\n",
    "4. Instantiate a 3D convolutional neural network.\n",
    "5. Choose a loss function and metrics to use.\n",
    "6. Train on part of the data.\n",
    "7. Evaluate on the rest of the data.\n",
    "\n",
    "## Google Colaboratory\n",
    "\n",
    "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhBnt2WdDlx9"
   },
   "outputs": [],
   "source": [
    "!pip install nobrainer nilearn\n",
    "!export TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht_CGSk1Dkt3",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVCchp9uDkt3"
   },
   "source": [
    "# Get sample features and labels\n",
    "\n",
    "We use 9 pairs of volumes for training and 1 pair of volumes for evaluation. Many more volumes would be required to train a model for any useful purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpqTxNu4Dkt4",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "csv_path = nobrainer.utils.get_data()\n",
    "filepaths = nobrainer.io.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScgF78rmDkt4"
   },
   "source": [
    "# Convert medical images to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7lCL-55Ta4R",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nobrainer.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3zPyRlbTa4R",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "DT = Dataset(n_classes=1,\n",
    "             batch_size=2,\n",
    "             block_shape=(128, 128, 128),\n",
    "             n_epochs=n_epochs)\n",
    "\n",
    "dataset_train, dataset_eval = DT.from_files(paths=filepaths,\n",
    "                                            eval_size=0.1,\n",
    "                                            tfrecdir=\"data/binseg\",\n",
    "                                            shard_size=3,\n",
    "                                            augment=None,\n",
    "                                            shuffle_buffer_size=10,\n",
    "                                            num_parallel_calls=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c9d34",
   "metadata": {},
   "source": [
    "## Construct a U-Net model\n",
    "Here we'll train nobrainer's implementation of the U-Net model for biomendical image segmentation, based on https://arxiv.org/abs/1606.06650. Note that a useful segmentation model would need to be trained on *many* more examples than the 10 we are using here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8u_owicTa4T",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nobrainer.processing.segmentation import Segmentation\n",
    "from nobrainer.models import unet\n",
    "\n",
    "bem = Segmentation(unet, model_args=dict(batchnorm=True))\n",
    "bem.fit(dataset_train=dataset_train,\n",
    "        dataset_validate=dataset_eval,\n",
    "        epochs=n_epochs,\n",
    "        multi_gpu=True,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc1589",
   "metadata": {},
   "source": [
    "## Use the trained model to predict the binary brian mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWqLu2xFTa4U"
   },
   "outputs": [],
   "source": [
    "from nobrainer.volume import standardize\n",
    "\n",
    "image_path = filepaths[0][0]\n",
    "out = bem.predict(image_path, normalizer=standardize)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xJxR7Ddbd-0"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "plotting.plot_roi(out, bg_img=image_path, alpha=0.4, vmin=0, vmax=5, figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Save model\n",
    "2. Load model back as a class instance\n",
    "3. Perform prediction\n",
    "4. Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem.save(\"data/testsave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nobrainer.processing.segmentation import Segmentation\n",
    "bem = Segmentation.load(\"data/testsave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = filepaths[0][0]\n",
    "out = bem.predict(image_path, normalizer=standardize)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem.fit(dataset_train=dataset_train,\n",
    "        dataset_validate=dataset_eval,\n",
    "        epochs=1,\n",
    "        multi_gpu=True,\n",
    "        warm_start=True,\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_binary_segmentation.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
